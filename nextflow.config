/*
=========================================================================================
  		NANOME(Nanopore methylation) pipeline for Oxford Nanopore sequencing
=========================================================================================
 NANOME Analysis Pipeline.
 #### Homepage / Documentation
 https://github.com/TheJacksonLaboratory/nanome
 @Author   : Yang Liu
 @FileName : nextflow.config
 @Software : NANOME project
 @Organization : JAX Li Lab
----------------------------------------------------------------------------------------
*/

params {
	// nanome running software env for Conda, Docker and Singularity
	conda_base_dir='/opt/conda'  // sample: /home/liuya/anaconda3
	conda_name = "nanome"  // sample: /projects/li-lab/yang/anaconda3/envs/nanome
	conda_cache = 'local_conda_cache'

	docker_name = "liuyangzzu/nanome:latest"
	singularity_name = "docker://liuyangzzu/nanome:latest"
	singularity_cache = 'local_singularity_cache'
	// singularityBeforeScript = "module load singularity"

	tracedir = 'NANOME_trace'
	help = false

	// process and executor configurations
	executor = false
	queueSize = 50
	echo = false

	// Default input params for pipeline running
	// dsname = 'TestData'
	// input = 'https://raw.githubusercontent.com/TheJacksonLaboratory/nanome/master/inputs/test.demo.filelist.txt'
	dsname = false
	input = false
	outdir = "outputs"

	// Data type, can be human, ecoli, etc.
	genome = "hg38"
	type = "human"

	// slurm default parameters
	queue = 'gpu'
	qos = 'inference'
	time = '2h'
	memory = '32GB'
	gresOptions = 'gpu:v100:1' // false for no need for gpu resources

	// process default settings
	cacheStrategy = 'lenient'
	errorStrategy = 'terminate' // or 'ignore'

	// true if clean work dir when pipeline complete
	cleanCache = true // clean work dir after workflow finished
	cleanStep = true // clean after each process finished, optimize disk usage

	 // Defaults max resource
	max_memory                 = 300.GB
	max_cpus                   = 16
	max_time                   = 336.h

	// number of processors for a task
	processors = 8

	// for compute intensive jobs, we use processors*times as multiprocessing options
	highProcTimes = 4
	mediumProcTimes = 2 // for normal process speedup, e.g., Tombo, Nanopolish, etc.
	lowProcTimes = 1 // for large memory process, e.g., megalodon, use conservative time 1 is reasonable
	reduceProcTimes = 1  // can be 0.5 for reduce the process, e.g., nanopolish, resquiggle, may set to 0.5 for large scale data

	cleanAnalyses = false // true if clean previous analysis in fast5 inputs
	//##################################################################
	//############### Reserved by tools default settings ###############
	//##################################################################
	//##################################################################
	// Default tool running configuration, top 4 as default
	runNanopolish = true
	runMegalodon = true
	runDeepSignal = true
	runGuppy = true

	runTombo = false
	runDeepMod = false
	runMETEORE = false

	runCombine = true
	runBasecall = true
	runMethcall = true

	//##################################################################
	// Default consensus for NANOME, default is Megalodon and Nanopolish
	nanomeNanopolish = true
	nanomeMegalodon = true
	nanomeDeepSignal = false
	nanomeGuppy = false

	// if perform evaluations after callings
	outputQC = true // QC report for basecall
	outputIntermediate = false  // keep each batch outputs
	outputRaw = true // Raw combined outputs for each tool's format

	//======================================================
	//======================================================
	// Tools' specific additional options
	// Resquiggle specifications
	BasecallGroupName = "Basecall_1D_000" // Basecall ID name used by resquiggle
	BasecallSubGroupName = "BaseCalled_template"
	ResquiggleCorrectedGroup = "RawGenomeCorrected_000"
	tomboResquiggleOptions = ' ' // '--signal-length-range 0 500000  --sequence-length-range 0 50000', ref:  tombo resquiggle --print-advanced-arguments
	tomboMultiprocessRegionSize = 1000 // tombo methylation calling options

	// DeepSignal model
	DEEPSIGNAL_MODEL_TAR_GZ="model.CpG.R9.4_1D.human_hx1.bn17.sn360.v0.1.7+.tar.gz"
	DEEPSIGNAL_MODEL = 'model.CpG.R9.4_1D.human_hx1.bn17.sn360.v0.1.7+/bn_17.sn_360.epoch_9.ckpt'

	// DeepMod options
	// DeepMod default used model specifications
	DeepModGithub = "https://github.com/WGLab/DeepMod/archive/refs/tags/v0.1.3.tar.gz"
	DEEPMOD_RNN_MODEL = "rnn_conmodC_P100wd21_f7ne1u0_4/mod_train_conmodC_P100wd21_f3ne1u0"
	DEEPMOD_CLUSTER_MODEL = "na12878_cluster_train_mod-keep_prob0.7-nb25-chr1/Cg.cov5.nb25"
	useDeepModCluster = true
	moveOption = true // options of Guppy basecalled input for DeepMod, empty for Albacore usage
	chrSet = true //used by DeepMod, true is default, will use '  ' means all Human chromosomes, else need to specify such as chr1,chr2

	// Guppy model specificatoins
	guppyDir = false // default is in PATH var
	// Suggested model by Guppy basecall
	GUPPY_BASECALL_MODEL = "dna_r9.4.1_450bps_hac.cfg"
	// Suggested model by Guppy methcall
	// GUPPY_METHCALL_MODEL="dna_r9.4.1_450bps_modbases_dam-dcm-cpg_hac.cfg" //  for Guppy v4.2.2
	GUPPY_METHCALL_MODEL = 'dna_r9.4.1_450bps_modbases_5mc_hac.cfg'

	// The suggested model and options by Megalodon
	MEGALODON_MODEL_FOR_GUPPY_CONFIG = "res_dna_r941_min_modbases_5mC_v001.cfg"
	GUPPY_TIMEOUT = 300
	READS_PER_GUPPY_BATCH = 100
	SAMTOOLS_PATH = "samtools"

	// METEORE Github
	METEOREGithub = "https://github.com/comprna/METEORE/archive/refs/tags/v1.0.0.tar.gz"
	METEORE_Dir = "METEORE-1.0.0"

	//##################################################################
	//############### Reserved by google cloud computing ###############
	//##################################################################
	//##################################################################
	// Google cloud computing configurations defaults
	// used for google computing platform, ref: https://cloud.google.com/compute/docs/regions-zones#available
	// for exit code error info, ref: https://cloud.google.com/life-sciences/docs/troubleshooting#error_codes
	googleProjectName = 'jax-nanopore-01'
	googleLocation = 'us'
	googleRegion = 'us-east1'
	bootDiskSizeCloud = 20.GB
	preemptibleCloud = true // save costs using preemptible way
	networkCloud = 'default'
	subnetworkCloud = 'default'

	maxRetries = 5

	machineType = "n1-standard-8" // or "n1-highmem-16", ref: https://cloud.google.com/compute/docs/general-purpose-machines#n1-shared-core
	gpuType = 'nvidia-tesla-p100' // lower price than v100, ref: https://cloud.google.com/compute/gpus-pricing
	gpuNumber = 1
	highmemMachineType = "n1-highmem-16"  // n1-highmem-16 16 104, n1-standard-8 8 30

	lowDiskSize = 100.GB // for test and check
	midDiskSize = 150.GB // for methylation
	highDiskSize = 200.GB // for untar, basecall and resquiggle

	// Lifebit cloudOS config used, please set to 'conf/executors/lifebit.config'
	config = 'conf/executors/local.config'

	//##################################################################
	//########################### End of block #########################
	//##################################################################
	//##################################################################
}

try {
  	// Default online input used by all pipeline, input were located in zenodo,
	// ref: https://zenodo.org/record/5513090
	includeConfig 'conf/executors/zenodo_input.config'
} catch (Exception e) {
  	System.err.println("WARNING: Could not load NANOME config file: conf/executors/zenodo_input.config")
}

// Running on different platforms
profiles {
	// Default profile used when user not specify, ref: https://www.nextflow.io/docs/latest/config.html#config-profiles
	// For Lifebit CloudOS running, please set --config as 'conf/executors/lifebit.config'
	standard { includeConfig params.config }

	test { includeConfig 'conf/examples/test.config' }

	test_human { includeConfig 'conf/examples/test_human.config' }

	conda {
		process.conda = params.conda_name
		conda.cacheDir = params.conda_cache
	}

	docker {
		process.container = params.docker_name
		// process.containerOptions = params.containerOptions
		docker{
			enabled = true
			// runOptions = params.containerOptions // pass CUDA var to process for docker container, --gpus all, ref:https://docs.docker.com/engine/reference/commandline/run/
			// temp = 'auto'
			envWhitelist = 'CUDA_VISIBLE_DEVICES' // Ref: https://www.nextflow.io/docs/latest/config.html#scope-docker
		}
	}

	singularity {
		process {
			container = params.singularity_name
			//beforeScript = "module load singularity"
		}

		singularity {
			enabled = true
			autoMounts = true
			cacheDir = params.singularity_cache
			envWhitelist = 'CUDA_VISIBLE_DEVICES' // Ref: https://github.com/nextflow-io/nextflow/issues/776
		}
	}

	hpc { // general hpc configuration
		process {
			executor = 'slurm'
			queue = params.queue
			cpus = params.processors
			memory = params.memory
			time = params.time
			clusterOptions = "-q ${params.qos}  ${params.gresOptions == false ? '  ': '--gres=' + params.gresOptions}"
		}
	}

	sumner { // jax hpc sumner configuration
		params{
			max_cpus = 72
			max_memory = 768.GB

			queue = 'compute,high_mem' // sumner support multiple partitions
			qos = 'batch'
			processors = 8
			memory = '32GB'
			time = '2h'
			gresOptions = false
		}

		process{
			executor = "slurm"
			module = "slurm:singularity"

			queue = params.queue
			cpus = params.processors
			memory = params.memory
			time = params.time
			clusterOptions = "-q ${params.qos}"
		}
	}

	winter { // jax hpc winter configuration
		params{
			max_cpus = 72
			max_memory = 768.GB

			queue = 'gpu' // winter only have one partition
			qos = 'inference' // or use training, time can be 14 days
			processors = 8
			memory = '32GB'
			time = '2h'
			gresOptions = 'gpu:v100:1' // false if no gpu needed
		}

		process{
			executor = "slurm"
			module = "slurm:singularity"

			queue = params.queue
			cpus = params.processors
			memory = params.memory
			time = params.time
			clusterOptions = "-q ${params.qos}  ${params.gresOptions == false ? '  ': '--gres=' + params.gresOptions}"
		}
	}

	// Google cloud computing platform
	google {
		params{ //overide default params for GCP
			errorStrategy = 'ignore'
		}
		executor {
			name = 'google-lifesciences'
			pollInterval = '30 sec'
		}

		google {
			project = params.googleProjectName
			// use region instead of zone, a region contains many zones: zone = 'us-east1-c'
			region = params.googleRegion
			location = params.googleLocation
			lifeSciences.debug = true
			lifeSciences.preemptible = params.preemptibleCloud
			lifeSciences.usePrivateAddress = false
			lifeSciences.sshDaemon = true
			lifeSciences.bootDiskSize = params.bootDiskSizeCloud
			enableRequesterPaysBuckets = true
			lifeSciences.network = params.networkCloud
			lifeSciences.subnetwork = params.subnetworkCloud
		}

		// Include nanome input from google cloud params
		// includeConfig 'conf/gc_params.config'
		process {
			// Machine types ref: https://cloud.google.com/solutions/sql-server-performance-tuning-compute-engine.pdf?hl=en
			machineType = params.machineType
			disk = params.midDiskSize
			maxRetries = params.maxRetries
			echo = params.echo
			// Ref: https://cloud.google.com/life-sciences/docs/troubleshooting
			errorStrategy = {task.attempt == process.maxRetries ? params.errorStrategy :  task.exitStatus in [10, 14] ? 'retry' : params.errorStrategy }

			withName: 'Megalodon|Resquiggle|DeepSignal|DeepMod' { // allocate highmem machine type, if facing large data
				machineType = params.highmemMachineType
			}

			withName: 'EnvCheck|Basecall|Guppy|Megalodon' { // allocate gpu
				accelerator = [request:  params.gpuNumber, type: params.gpuType]
				beforeScript = "export CUDA_VISIBLE_DEVICES=0" // pass CUDA var to process, since GCP do not export it
			}

			withName: 'Untar|Basecall|Guppy|Resquiggle' { // allocate high disk size
				disk = params.highDiskSize
			}
		}
	}
}

env {
	PATH = ! params.guppyDir ? '$PATH': ["${params.guppyDir}/bin", '$PATH'].join(':')
	numProcessor = "${params.processors}"
}

process {
	cache = params.cacheStrategy
	errorStrategy = params.errorStrategy
	echo = params.echo
}

executor {
	name = params.executor
	queueSize = params.queueSize
}


dag {
  file = "${params.tracedir}/NANOME_dag_${params.dsname}.svg"
}

report {
  file = "${params.tracedir}/NANOME_report_${params.dsname}.html"
}

timeline {
  file = "${params.tracedir}/NANOME_timeline_${params.dsname}.html"
}

trace {
  file = "${params.tracedir}/NANOME_trace_${params.dsname}.txt"
}

manifest {
	name = 'TheJacksonLaboratory/nanome'
	author = 'Yang Liu'
	homePage = 'https://github.com/TheJacksonLaboratory/nanome'
	description = 'NANOME (Nanopore methylation) pipeline for Oxford Nanopore sequencing'
	mainScript = 'main.nf'
	nextflowVersion = '>=20.07.1'
	version = '1.3.5'
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
  if (type == 'memory') {
    try {
      if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'time') {
    try {
      if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'cpus') {
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}
