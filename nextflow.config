/*
=========================================================================================
  		NANOME(Nanopore methylation) pipeline for Oxford Nanopore sequencing
=========================================================================================
 NANOME Analysis Pipeline.
 #### Homepage / Documentation
 https://github.com/LabShengLi/nanome
 @Author   : Yang Liu
 @FileName : nextflow.config
 @Software : NANOME project
 @Organization : JAX Sheng Li Lab
----------------------------------------------------------------------------------------
*/

params {
	// nanome running software env for Conda, Docker and Singularity
	conda_base_dir='/opt/conda'  // sample: /home/liuya/anaconda3
	conda_name = "nanome"  // sample: /projects/li-lab/yang/anaconda3/envs/nanome
	conda_cache = 'local_conda_cache'

	docker_name = "liuyangzzu/nanome:v2.0.2"
	singularity_name = "docker://liuyangzzu/nanome:v2.0.2"
	singularity_cache = 'local_singularity_cache'

	containerOptions = null // or "--gpus all" for docker

	tombo_docker_name = "liuyangzzu/nanome:v1.4"
	clair3_docker_name = "hkubal/clair3:latest"

	// process and executor configurations
	executor = null
	queueSize = 50
	tracedir = 'NANOME_trace'
	help = false
	echo = false
	cacheStrategy = 'lenient'
	errorStrategy = 'ignore' // or 'ignore' 'terminate'
	maxRetries = 5
	// number of processors for a task
	processors = 2

	// Default input params for pipeline running
	// dsname = 'TestData'
	// input = 'https://raw.githubusercontent.com/LabShengLi/nanome/master/inputs/test.demo.filelist.txt'
	dsname = null
	input = null
	outdir = "results"

	// Data type, can be human, ecoli, etc.
	genome = "hg38"
	dataType = null // default is infered by NANOME
	chrSet = null // chomosommes used, default will apply to Human/Ecoli chromosomes, else need to specify such as 'chr1 chr2'

	cleanAnalyses = false // true if clean previous analysis in fast5 inputs
	deduplicate = false // true if deduplicate read-level outputs for tools
	sort = false // true if sort read level unified outputs

	// true if clean work dir when pipeline complete
	cleanup = false // clean work dir after workflow finished
	cleanStep = true // clean after each process finished, optimize disk usage
	//##################################################################
	//############### Reserved by tools default settings ###############
	//##################################################################
	//##################################################################
	// Default tool running configuration, top 4 as default
	runNanopolish 	= true
	runMegalodon 	= true
	runDeepSignal 	= true
	runGuppy 		= false
	runGuppyGcf52ref= false  // Guppy readlevel extract software, not certified by us
	runNANOME 		= true // NANOME concensus
	runNewTool		= false // run new added tool in interface

	newModuleConfigs = null

	runTombo 	= false
	runDeepMod 	= false
	runMETEORE 	= false

	runBasecall = true // even user provided basecalled input, this step need to run for prepare input for later steps
	skipBasecall= false // if user prepared basecalled input, want to skip basecall processing
	runMethcall = true
	runCombine 	= true

	// if perform evaluations after callings
	outputQC = true // output the QC report for basecall
	skipQC = false // if skip QC analysis
	outputIntermediate = false  // if keep each batch outputs
	outputRaw = true // if output the raw combined outputs for each tool's format
	outputGenomeBrowser = false
	outputBam = false // if output basecalled merged bam
	outputONTCoverage = false // if output ONT coverage

	// meth type
	hmc = false // true if running 5hmc for megalodon
	phasing = false // true if running clair3 and whatshap for phasing
	ctg_name = null // variant calling region
	filter_fail_fq = false // if filter out failed fastq files

	//======================================================
	//======================================================
	// Tools' specific additional options
	// for compute intensive jobs, we use processors*times as multiprocessing options
	highProcTimes = 4
	mediumProcTimes = 2 // for normal process speedup, e.g., Tombo, Nanopolish, etc.
	lowProcTimes = 1 // for large memory process, e.g., megalodon, use conservative time 1 is reasonable
	reduceProcTimes = 1  // can be 0.5 for reduce the process, e.g., nanopolish, resquiggle, may set to 0.5 for large scale data but limit memory

	// File name
	GENOME_FN = "ref.fasta"
	CHROM_SIZE_FN = "chrom.sizes"

	// Resquiggle specifications
	BasecallGroupName = "Basecall_1D_000" // Basecall ID name used by resquiggle
	BasecallSubGroupName = "BaseCalled_template"
	ResquiggleCorrectedGroup = "RawGenomeCorrected_000"
	tomboResquiggleOptions = null // '--signal-length-range 0 500000  --sequence-length-range 0 50000', ref:  tombo resquiggle --print-advanced-arguments
	tomboMultiprocessRegionSize = 1000 // tombo methylation calling options

	// DeepSignal model names
	deepsignalDir = null // default is get model online, or specify the name of model dir
	DEEPSIGNAL_MODEL_DIR = 'model.CpG.R9.4_1D.human_hx1.bn17.sn360.v0.1.7+'
	DEEPSIGNAL_MODEL = 'bn_17.sn_360.epoch_9.ckpt'

	// DeepMod options
	// DeepMod default used model specifications
	DeepModGithub = "https://github.com/WGLab/DeepMod/archive/refs/tags/v0.1.3.tar.gz"
	DEEPMOD_RNN_MODEL = "rnn_conmodC_P100wd21_f7ne1u0_4/mod_train_conmodC_P100wd21_f3ne1u0"
	DEEPMOD_CLUSTER_MODEL = "na12878_cluster_train_mod-keep_prob0.7-nb25-chr1/Cg.cov5.nb25"
	useDeepModCluster = false
	moveOption = false // options of move table

	// Guppy model specificatoins
	guppyDir = null // default is in PATH var
	// Suggested model by Guppy basecall
	GUPPY_BASECALL_MODEL = "dna_r9.4.1_450bps_hac.cfg"
	// Suggested model by Guppy methcall
	// GUPPY_METHCALL_MODEL="dna_r9.4.1_450bps_modbases_dam-dcm-cpg_hac.cfg" //  for Guppy v4.2.2
	// GUPPY_METHCALL_MODEL = 'dna_r9.4.1_450bps_modbases_5mc_hac.cfg' // for Guppy v6.0.1
	GUPPY_METHCALL_MODEL = 'dna_r9.4.1_450bps_modbases_5mc_cg_hac.cfg'

	// Megalodon model and options
	rerio = false
	rerioDir = null // default is online rerio github model
	rerioGithub = 'https://github.com/nanoporetech/rerio'
	MEGALODON_MODEL = "res_dna_r941_min_modbases_5mC_v001.cfg"

	remoraModel = 'dna_r9.4.1_e8'

	GUPPY_TIMEOUT = 500  // For CPU running for Megalodon, it may need be enlarged, or else Megalodon will be empty outputs
	READS_PER_GUPPY_BATCH = 100
	SAMTOOLS_PATH = "samtools"

	// METEORE Github
	METEOREDir = false // default is online github, or else specify locations
	METEORE_GITHUB_ONLINE = "https://github.com/comprna/METEORE/archive/refs/tags/v1.0.0.tar.gz"
	METEOREDirName = "METEORE-1.0.0"

	// concensus model
	NANOME_MODEL = 'NANOME3T' // or 'NA12878', 'NA12878_XGBoost_NA_top3'
	NANOME_CONSENSUS_TOOLS = 'Nanopolish Megalodon DeepSignal' // or 'Megalodon DeepSignal' for NANOME2T

	// Lifebit cloudOS config used, please set to 'conf/executors/lifebit.config'
	config = null // 'conf/executors/local.config'
}

try {
	// Include input files from google cloud storage
	includeConfig 'conf/executors/gcp_input.config'
} catch (Exception e) {
  	System.err.println("WARNING: Could not load config file: conf/executors/gcp_input.config")
}

// Running on different platforms
profiles {
	// Default profile used when user not specify, ref: https://www.nextflow.io/docs/latest/config.html#config-profiles
	// For Lifebit CloudOS running, please set --config as 'conf/executors/lifebit.config'
	standard { if (params.config) {includeConfig params.config} }

	test { includeConfig 'conf/examples/test.config' }

	test_human { includeConfig 'conf/examples/test_human.config' }

	conda {
		process.conda = params.conda_name
		conda.cacheDir = params.conda_cache
	}

	docker {
		params {
			containerOptions = null // users using GPU need to set to "--gpus all"
		}
		process {
			container = params.docker_name
			containerOptions = params.containerOptions // or "--gpus all" Note: this is not compatible with GitHub citest/naive docker users

			withName: 'Tombo|DeepMod|METEORE' {
				container = params.tombo_docker_name
			}
			withName: 'Clair3' {
				container = params.clair3_docker_name
			}
		}

		docker{
			enabled = true
			// runOptions = params.containerOptions // pass CUDA var to process for docker container, --gpus all, ref:https://docs.docker.com/engine/reference/commandline/run/
			// temp = 'auto'
			envWhitelist = 'CUDA_VISIBLE_DEVICES' // Ref: https://www.nextflow.io/docs/latest/config.html#scope-docker
		}
	}

	singularity {
		params {
			containerOptions = "--nv"
		}
		process {
			container = params.singularity_name
			containerOptions = params.containerOptions // "--nv"

			withName: 'Tombo|DeepMod|METEORE' {
				container = "docker://${params.tombo_docker_name}"
			}
			withName: 'Clair3' {
				container = "docker://${params.clair3_docker_name}"
			}
		}

		singularity {
			enabled = true
			autoMounts = true
			cacheDir = params.singularity_cache
			envWhitelist = 'CUDA_VISIBLE_DEVICES' // Ref: https://github.com/nextflow-io/nextflow/issues/776
		}
	}

	hpc { // general hpc configuration
		params {
			// hpc slurm default parameters
			queue = 'gpu'
			qos = 'inference'
			processors = 4
			memory = '32GB'
			time = '5h'
			gresOptions = 'gpu:v100:1' // null/false for no need for gpu resources

			// Defaults max resource
			max_memory                 = 300.GB
			max_cpus                   = 16
			max_time                   = 336.h

			queueSize = 50	// max number of job submit
		}
		process {
			executor = 'slurm'

			queue = params.queue
			qos = params.qos
			cpus = params.processors
			memory = params.memory
			time = params.time
			clusterOptions = "-q ${params.qos}  ${params.gresOptions ?  '--gres=' + params.gresOptions : '  '}  "
		}
		executor {
			queueSize = params.queueSize
		}
	}

	sumner { // jax hpc sumner configuration
		params{
			max_cpus = 72
			max_memory = 768.GB

			queue = 'compute,high_mem' // sumner support multiple partitions
			qos = 'batch'

			processors = 8
			memory = '32GB'
			time = '2h'
			gresOptions = null

			queueSize = 300	// max number of job submit
		}

		process{
			executor = "slurm"
			module = "slurm:singularity"

			queue = params.queue
			cpus = params.processors
			memory = params.memory
			time = params.time
			clusterOptions = "-q ${params.qos}  ${params.gresOptions ?  '--gres=' + params.gresOptions: '  ' }  "
		}

		executor {
			queueSize = params.queueSize
		}
	}

	winter { // jax hpc winter configuration
		params{
			max_cpus = 72
			max_memory = 768.GB

			queue = 'gpu' // winter only have one partition
			qos = 'inference' // or use training, time can be 14 days
			processors = 8
			memory = '32GB'
			time = '6h'
			gresOptions = 'gpu:v100:1' // null/false if no gpu needed
			queueSize = 24
		}

		process{
			executor = "slurm"
			module = "slurm:singularity"

			queue = params.queue
			cpus = params.processors
			memory = params.memory
			time = params.time
			clusterOptions = "-q ${params.qos}  ${params.gresOptions  ? '--gres=' + params.gresOptions : '  '}"
		}

		executor {
			queueSize = params.queueSize
		}
	}

	// Google cloud computing platform
	google {
		params{
			//##################################################################
			//############### Reserved by google cloud computing ###############
			//##################################################################
			//##################################################################
			// Google cloud computing configurations defaults
			// used for google computing platform, ref: https://cloud.google.com/compute/docs/regions-zones#available
			// for exit code error info, ref: https://cloud.google.com/life-sciences/docs/troubleshooting#error_codes
			projectCloud = null // e.g., 'jax-nanopore-01'
			locationCloud = 'us'
			regionCloud = 'us-east1'
			zoneCloud = null // use region instead of zone can get GPU from more zones
			debugCloud = true
			sshDaemonCloud = true
			bootDiskSizeCloud = 30.GB
			preemptibleCloud = true // save costs using preemptible way
			networkCloud = 'default'
			subnetworkCloud = 'default'

			// Example: "n1-standard-8", or custom-[NUMBER_OF_CPUS]-[AMOUNT_OF_MEMORY]
			machineType = null //"n1-standard-8" or "n1-highmem-8", ref: https://cloud.google.com/compute/docs/general-purpose-machines#n1-shared-core
			processors = 8  // for 8 cpus, max mem is 52 GB in GCP. Memory must be between 0.9 GB per vCPU, up to 6.5 GB per vCPU.
			memory = '30 GB'
			gpuType = 'nvidia-tesla-p100' // or 'nvidia-tesla-t4', lower price than 'nvidia-tesla-v100', ref: https://cloud.google.com/compute/gpus-pricing
			gpuNumber = 1

			lowDiskSize = 100.GB // for test and check
			midDiskSize = 150.GB // for methylation
			highDiskSize = 200.GB // for untar, basecall and resquiggle, need much disk sizes

			//overide default params for GCP
			errorStrategy = 'ignore'
		}

		executor {
			name = 'google-lifesciences'
			pollInterval = '30 sec'
		}

		google {
			project = params.projectCloud

			// use region instead of zone, a region contains many zones: zone = 'us-east1-c'
			location = params.locationCloud
			region = params.regionCloud
			zone = params.zoneCloud

			lifeSciences.debug = params.debugCloud
			lifeSciences.preemptible = params.preemptibleCloud
			lifeSciences.sshDaemon = params.sshDaemonCloud
			lifeSciences.bootDiskSize = params.bootDiskSizeCloud

			lifeSciences.network = params.networkCloud
			lifeSciences.subnetwork = params.subnetworkCloud

			lifeSciences.usePrivateAddress = false
			enableRequesterPaysBuckets = true
		}

		env {
			PATH = null
		}

		// Include nanome input from google cloud params
		// includeConfig 'conf/gc_params.config'
		process {
			// Machine types ref: https://cloud.google.com/solutions/sql-server-performance-tuning-compute-engine.pdf?hl=en
			// or: https://cloud.google.com/compute/docs/general-purpose-machines#n1-standard
			machineType = params.machineType
			cpus = params.processors
			memory = params.memory
			time = null
			disk = params.midDiskSize
			maxRetries = params.maxRetries
			echo = params.echo
			// Ref: https://cloud.google.com/life-sciences/docs/troubleshooting
			errorStrategy = {task.attempt == process.maxRetries ?
					params.errorStrategy :  task.exitStatus in [2, 10, 14] ? 'retry' : params.errorStrategy }

			withName: 'EnvCheck' {
				// download Rerio model may encounter exitstatus:1, need retry
				errorStrategy = {task.attempt == process.maxRetries ?
						params.errorStrategy :  task.exitStatus in [1, 2, 10, 14] ? 'retry' : params.errorStrategy }
			}

			withName: 'EnvCheck|Basecall|Guppy|Megalodon' { // allocate gpu
				accelerator = [request:  params.gpuNumber, type: params.gpuType]
				beforeScript = "export CUDA_VISIBLE_DEVICES=0" // pass CUDA var to process, since GCP do not export it
				containerOptions = { workflow.containerEngine == "singularity" ? '--nv':
       									( workflow.containerEngine == "docker" ? '--gpus all': null ) }
			}

			withName: 'Untar|Basecall|Guppy|Resquiggle' { // allocate high disk size
				disk = params.highDiskSize
			}
		}
	}
}

env {
	// Local test, specify the guppy dir in PATH
	PATH = ! params.guppyDir ? '$PATH': ["${params.guppyDir}/bin", '$PATH'].join(':')
}

process {
	cache = params.cacheStrategy
	errorStrategy = params.errorStrategy
	echo = params.echo
	maxRetries = params.maxRetries

	cpus = 2
	memory = '4GB'
	time = '5h'

	withName: 'EnvCheck' {
		// allow retry if download Rerio model failed
		errorStrategy = {task.attempt >= process.maxRetries ?
			params.errorStrategy :  task.exitStatus in [1] ? 'retry' : params.errorStrategy }
	}
}

executor {
	name = params.executor
	queueSize = params.queueSize
}

cleanup = params.cleanup

dag {
  file = "${params.tracedir}/NANOME_dag_${params.dsname}.svg"
}

report {
  file = "${params.tracedir}/NANOME_report_${params.dsname}.html"
}

timeline {
  file = "${params.tracedir}/NANOME_timeline_${params.dsname}.html"
}

trace {
  file = "${params.tracedir}/NANOME_trace_${params.dsname}.txt"
}

manifest {
	name = 'LabShengLi/nanome'
	author = 'Yang Liu'
	homePage = 'https://github.com/LabShengLi/nanome'
	description = 'NANOME (Nanopore methylation) pipeline for Oxford Nanopore sequencing by JAX Li Lab'
	mainScript = 'main.nf'
	nextflowVersion = '>=20.07.1'
	version = '2.0.0'
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
  if (type == 'memory') {
    try {
      if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'time') {
    try {
      if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if (type == 'cpus') {
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}
